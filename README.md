# Scaling Laws for BERT in Low-Resource Settings

Data (corpora and datasets) and models from our work *Scaling Laws for BERT in Low-Resource Settings* accepted at ACL2023 Findings.


Authors
-----------
Gorka Urbizu [1], Iñaki San Vicente [1], Xabier Saralegi [1],
Rodrigo Agerri [2] and Aitor Soroa [2]

Affiliation of the authors: 

[1] orai NLP Technologies

[2] HiTZ Center - Ixa, University of the Basque Country UPV/EHU



Licensing
-------------

Copyright (C) by Orai NLP Technologies. 

The corpora, datasets and models created in this work, are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International

International License (CC BY-SA-NC 4.0). To view a copy of this license, visit [http://creativecommons.org/licenses/by-nc-sa/4.0/](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.eu).




Acknowledgements
-------------------
If you use this benchmark please cite the following paper:

- G. Urbizu, I. San Vicente, X. Saralegi, R. Agerri, A. Soroa. Scaling Laws for BERT in Low-Resource Settings. Findings of the Association for Computational Linguistics: ACL 2023. July, 2023. Toronto, Canada



Contact information
-----------------------
Gorka Urbizu, Iñaki San Vicente: {g.urbizu,i.sanvicente}@orai.eus
